{"cells":[{"cell_type":"markdown","metadata":{},"source":["# REFERENCIAS"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"63c1d5bb-ce28-4cf7-ad20-366c2fba2e83","showTitle":false,"title":""}},"outputs":[],"source":["#https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html\n","\n","# FUNÇÕES EM SCALA PARA NORMALIZAÇÃO DE ARRAYS\n","#https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/\n","\n","# CALCULO DE MEMÓRIA DE EXECUTORES\n","#https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html\n","\n","# AJUSTE DE PARTIÇÕES\n","#https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/\n","\n","# ESTIMATIVA DE MEMÓRIA USADA PELO SPARK\n","#https://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7dab4114-6b7b-4d08-aec0-9a7d90b3bffa","showTitle":false,"title":""}},"source":["# IMPORTS"]},{"cell_type":"markdown","metadata":{},"source":["### GENERAL IMPORTS"]},{"cell_type":"code","execution_count":1,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d9d60deb-380e-4f9c-b29d-d0bfb8ef35a4","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"]}],"source":["\n","import json\n","import bson as bson\n","import string as s\n","import pandas as pd\n","import os.path,inspect, re \n","import random as r\n","from types import SimpleNamespace\n","import sys\n","import pathlib\n","import time\n","import os\n","import zipfile\n","import concurrent.futures\n","from pymongo import MongoClient\n","import databricks.koalas as ks\n","\n","\n","from pyspark.sql.types import *\n","\n","from pyspark.sql import types as T \n","from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","from datetime import datetime, timedelta \n","import pyspark\n","from pyspark.sql import SQLContext  \n","from pyspark.sql import *\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["sc = \"\"\n","spark = \"\"\n","def localInit():\n","    sc = pyspark.SparkContext()\n","    spark = pyspark.sql.SparkSession(sc)\n","    spark.conf.set('spark.Builder.enableHiveSupport',True)\n","    spark.conf.set(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")\n","    spark.conf.set('spark.sql.execution.arrow.pyspark.enabled' , False)\n","    spark.conf.set('spark.sql.caseSensitive', True)\n","    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n","\n","localInit()"]},{"cell_type":"markdown","metadata":{},"source":["## GET MDB SCHEMA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def getMdbSchema(sampleSize = 50000, mode=\"local\",col=None):\n","    DATA_PATH = \"local_path\" \n","    if mode ==\"local\":\n","        with open(DATA_PATH) as f:\n","            DATA_SCHEMA = T.StructType.fromJson(json.load(f))\n","\n","    else:\n","        DATA = spark.read\\\n","        .format( \"com.mongodb.spark.sql.DefaultSource\")\\\n","        .option('spark.mongodb.input.sampleSize', sampleSize)\\\n","        .option(\"database\", \"database\")\\\n","        .option(\"spark.mongodb.collection.collection\", col)\\\n","        .load().schema\n","\n","        DATA\n","\n","\n","        with open(\"schema.json\", \"w\") as f:\n","            json.dump(DATA.jsonValue(), f)\n","\n","        with open(\"schema.json\") as f:\n","            DATA_SCHEMA = T.StructType.fromJson(json.load(f))\n","    return DATA_SCHEMA"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2d62c8e6-bbd1-40c6-9333-f970d7ebeca3","showTitle":false,"title":""}},"source":["## MDB QUERY"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["URI = \"\"\n","def mdbBuild(URI=None, DATABASE=None,COLLECTION=None):\n","    client = MongoClient(URI)\n","    db = client.get_database(DATABASE)\n","    query_base = db.get_collection(COLLECTION)\n","    return query_base\n"]},{"cell_type":"markdown","metadata":{},"source":["## DIR FUNCTIONS"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1be7b5a5-91bf-4184-9ea7-f96912fe43f0","showTitle":false,"title":""}},"outputs":[],"source":["\n","        \n","def countDir(self): \n","    dcFiles = dbutils.fs.ls(self.DIR)\n","    df = spark.createDataFrame(sc.parallelize(dcFiles))\n","    count = df.count()\n","    displayHTML(\"DIRECTORY SET AS: ...  %s\" % self.DIR)\n","    displayHTML(\"TOTAL OF FILES FOUND AFTER SCANNING : ... %s\" % count)   \n","\n","def graphDir(self):\n","    dcFiles = dbutils.fs.ls(self.DIR)\n","    df = spark.createDataFrame(sc.parallelize(dcFiles))\n","    df = df.sort(df.size)\n","    count = df.count()\n","    display(df)\n","\n","def descDir(self):\n","    #origin\n","    dcFiles = dbutils.fs.ls(self.dc)\n","    df = spark.createDataFrame(sc.parallelize(dcFiles))\n","    df = df.describe()\n","    display(df)\n","\n","def filesDir(self):\n","    #origin\n","    dcFiles = dbutils.fs.ls(self.DIR)\n","    return dcFiles       \n","    \n","\n","\n","def getInterByIdSingle(self,id=None):\n","    col = st.findByMongo(COLLECTION='collection')\n","    col_filter = [\n","        {\n","            '$match': {\n","                '_id': '%s' % id\n","            }\n","        }\n","                ]\n","    result = col.aggregate(col_filter)\n","    result = list(result)\n","    #UTILIZAR ALGUMA FORMA DE PERSISTÊNCIA EM MEMÓRIA PARA ACUMULAR OS RESULTADOS\n","    self.LIST.append(result)\n","    size = len(self.INTER_LIST)\n","    print(size)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["%md ALTERNATIVE PLOT"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"SparkSet","notebookOrigID":1279660130152254,"widgets":{"SCHEMA_STATUS":{"currentValue":"LOADED","nuid":"0b882202-204c-4d41-b8f5-9204fa5d46c6","widgetInfo":{"defaultValue":"LOADED","label":null,"name":"SCHEMA_STATUS","options":{"choices":["LOADED","NOT LOADED"],"widgetType":"dropdown"},"widgetType":"dropdown"}}}},"interpreter":{"hash":"de2447fb81e3bdedb6496b06bacdb2c52fb1921ab17eb741c5f37bae09bc662a"},"kernelspec":{"display_name":"Python 3.9.10 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}
