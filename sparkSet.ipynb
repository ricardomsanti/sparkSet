{"cells":[{"cell_type":"markdown","metadata":{},"source":["# REFERENCIAS"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"63c1d5bb-ce28-4cf7-ad20-366c2fba2e83","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html\n","\n","# FUNÇÕES EM SCALA PARA NORMALIZAÇÃO DE ARRAYS\n","#https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/\n","\n","# CALCULO DE MEMÓRIA DE EXECUTORES\n","#https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html\n","\n","# AJUSTE DE PARTIÇÕES\n","#https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/\n","\n","# ESTIMATIVA DE MEMÓRIA USADA PELO SPARK\n","#https://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7dab4114-6b7b-4d08-aec0-9a7d90b3bffa","showTitle":false,"title":""}},"source":["# IMPORTS"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d9d60deb-380e-4f9c-b29d-d0bfb8ef35a4","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["\n","import json\n","import bson as bson\n","import string as s\n","import pandas as pd\n","import os.path,inspect, re \n","import random as r\n","from types import SimpleNamespace\n","import sys\n","import pathlib\n","import time\n","import os\n","import zipfile\n","import concurrent.futures\n","from pymongo import MongoClient\n","import databricks.koalas as ks\n","\n","\n","from pyspark.sql.types import *\n","\n","from pyspark.sql import types as T \n","from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","from datetime import datetime, timedelta \n","import pyspark\n","from pyspark.sql import SQLContext  \n","\n","spark.conf.set('spark.Builder.enableHiveSupport',True)\n","spark.conf.set(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")\n","spark.conf.set('spark.sql.execution.arrow.pyspark.enabled' , False)\n","spark.conf.set('spark.sql.caseSensitive', True)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\") \n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2d62c8e6-bbd1-40c6-9333-f970d7ebeca3","showTitle":false,"title":""}},"source":["# SPARK_TOOLS"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1be7b5a5-91bf-4184-9ea7-f96912fe43f0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["\n","\n","class SparkTools:\n","    def __init__(self, CREDITOR=None,DATABASE=None,COLLECTION=None,DIR=None):\n","        self.CREDITOR = CREDITOR\n","        self.DATABASE = DATABASE\n","        self.COLLECTION = COLLECTION\n","        self.SCHEMA = self.getSchema()\n","        self.URI = \"URI\"\n","        self.LIST = [] \n","        self.DIR = DIR \n","\n","    \n","        \n","        \n","    def getSchema(self,sampleSize = 50000, mode=\"local\",col=\"collection\"):\n","        DATA_PATH = \"local_path\" \n","\n","        if mode ==\"local\":\n","          with open(DATA_PATH) as f:\n","            PersonSchema = T.StructType.fromJson(json.load(f))\n","\n","        else:\n","          DATA = spark.read\\\n","          .format( \"com.mongodb.spark.sql.DefaultSource\")\\\n","          .option('spark.mongodb.input.sampleSize', sampleSize)\\\n","          .option(\"database\", \"database\")\\\n","          .option(\"spark.mongodb.collection.collection\", \"collection\")\\\n","          .option(\"badRecordsPath\", \"/tmp/badRecordsPath\")\\\n","          .load().schema\n","\n","          DATA\n","\n","\n","          with open(\"schema.json\", \"w\") as f:\n","            json.dump(DATA.jsonValue(), f)\n","\n","          with open(\"schema.json\") as f:\n","            PersonSchema = T.StructType.fromJson(json.load(f))\n","        dbutils.widgets.dropdown(\"SCHEMA_STATUS\", \"LOADED\", [\"LOADED\", \"NOT LOADED\"])\n","        return PersonSchema\n"," \n","    def getCols(self):\n","        \n","        COLS = spark.read.schema(self.SCHEMA)\\\n","                .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n","                .option('spark.mongodb.input.database', self.DATABASE)\\\n","                .option('spark.mongodb.input.collection', self.COLLECTION)\\\n","                .load().columns\n","        return COLS\n","      \n","\n","    #realiza queries no banco MongoDB utilizando o driver em Python\n","    def findByMongo(self, DATABASE=\"database\", COLLECTION=\"col_person\"):\n","      client = MongoClient(self.collection)\n","      db = client.get_database(DATABASE)\n","      query = db.get_collection(COLLECTION)\n","      return query\n","\n","            \n","    def countDir(self): \n","        dcFiles = dbutils.fs.ls(self.DIR)\n","        df = spark.createDataFrame(sc.parallelize(dcFiles))\n","        count = df.count()\n","        displayHTML(\"DIRECTORY SET AS: ...  %s\" % self.DIR)\n","        displayHTML(\"TOTAL OF FILES FOUND AFTER SCANNING : ... %s\" % count)   \n","\n","    def graphDir(self):\n","        dcFiles = dbutils.fs.ls(self.DIR)\n","        df = spark.createDataFrame(sc.parallelize(dcFiles))\n","        df = df.sort(df.size)\n","        count = df.count()\n","        display(df)\n","\n","    def descDir(self):\n","        #origin\n","        dcFiles = dbutils.fs.ls(self.dc)\n","        df = spark.createDataFrame(sc.parallelize(dcFiles))\n","        df = df.describe()\n","        display(df)\n","\n","    def filesDir(self):\n","        #origin\n","        dcFiles = dbutils.fs.ls(self.DIR)\n","        return dcFiles       \n","      \n","\n","\n","    def getInterByIdSingle(self,id=None):\n","      col = st.findByMongo(COLLECTION='collection')\n","      col_filter = [\n","          {\n","              '$match': {\n","                  '_id': '%s' % id\n","              }\n","          }\n","                    ]\n","      result = col.aggregate(col_filter)\n","      result = list(result)\n","      #UTILIZAR ALGUMA FORMA DE PERSISTÊNCIA EM MEMÓRIA PARA ACUMULAR OS RESULTADOS\n","      self.LIST.append(result)\n","      size = len(self.INTER_LIST)\n","      print(size)\n","\n","    def threadExecute(self, FUNC = None, ITERABLE=None):\n","      \n","      #THREAD POOL EXECUTOR\n","      with concurrent.futures.ThreadPoolExecutor() as executor:\n","        futures = []\n","        #ITERABLE\n","        for ITEM in ITERABLE:\n","          #FUNCTION\n","          futures.append(executor.submit(FUNC, PARAM=ITEM))\n"]},{"cell_type":"markdown","metadata":{},"source":["## PLOT COLLECTION"]},{"cell_type":"markdown","metadata":{},"source":["### FUNNEL PLOT"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6935be69-5a4b-456c-8b30-37e86d76be3d","showTitle":false,"title":""}},"outputs":[],"source":["from plotly import graph_objects as go\n","\n","\n","fig = go.Figure()\n","name_list = [x for x in 'ABCDE']\n","step_list = [\"Website visit\", \"Downloads\", \"Potential customers\", \"Requested price\", \"invoice sent\", \"Finalized\"]\n","step_values = [10,20,30,40,50]\n","textinfo = [\"value+percent total\"]\n","\n","\n","trace_list =    [\n","                    {\n","                        \"name\":name_list[0],\n","                        \"orientation\": \"h\",\n","                        \"y\":step_list,\n","                        \"x\": step_values,\n","                        \"textinfo\": textinfo\n","                    },\n","                    {\n","                        \"name\":name_list[1],\n","                        \"orientation\": \"h\",\n","                        \"y\":step_list,\n","                        \"x\": step_values,\n","                        \"textinfo\": textinfo\n","                    },\n","                    {\n","                        \"name\":name_list[2],\n","                        \"orientation\": \"h\",\n","                        \"y\":step_list,\n","                        \"x\": step_values,\n","                        \"textinfo\": textinfo\n","                    },\n","                    {\n","                        \"name\":name_list[3],\n","                        \"orientation\": \"h\",\n","                        \"y\":step_list,\n","                        \"x\": step_values,\n","                        \"textinfo\": textinfo\n","                    }\n","                ]\n","\n","for x in trace_list:\n","    fig.add_trace(go.Funnel(\n","        name = x['name'],\n","        orientation=x['orientation'],\n","        y = list(x['y']),\n","        x = x['x'],\n","        textinfo = x['textinfo'][0]))\n","fig.show()\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["%md ALTERNATIVE PLOT"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a25b0ed7-0f9a-4461-ad0a-92233f46621a","showTitle":false,"title":""}},"outputs":[],"source":["import plotly.graph_objects as go\n","import pandas as pd\n","\n","\n","A ={\"nome\" : \"A\", \"tamanho\" : 12, \"parents\": \"empresas\"}\n","B ={\"nome\" : \"B\", \"tamanho\" : 5, \"parents\": \"empresas\"} \n","l1 = [A, B]\n","df = pd.DataFrame(l1)\n","\n","fig = go.Figure(\n","    go.Icicle(\n","        ids = df.nome,\n","        labels = df.tamanho,\n","        parents = df.parents,\n","        root_color=\"lightgrey\",\n","        tiling = dict(\n","            orientation='h'\n","        )\n","    )\n",")\n","fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Lambda Testing"]},{"cell_type":"markdown","metadata":{},"source":["#"]},{"cell_type":"markdown","metadata":{},"source":["%md map fund"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'koalas' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-7-04eae1e9625f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkoalas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkoalas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'koalas' is not defined"]}],"source":[" Built-In Functions\n"," – when()\n"," – expr()\n"," – lit()\n"," – split()\n"," – concat_ws()\n","–   substring()\n"," – translate()\n"," – regexp_replace()\n"," – overlay()\n"," – to_timestamp()\n"," – to_date()\n"," – date_format()\n"," – datediff()\n"," – months_between()\n"," – explode()\n"," – array_contains()\n"," – array()\n"," – collect_list()\n"," – collect_set()\n"," – create_map()\n"," – map_keys()\n"," – map_values()\n"," – struct()\n"," – countDistinct()\n"," – sum(), avg()\n"," – row_number()\n"," – rank()\n"," – dense_rank()\n"," – percent_rank()\n"," – typedLit()\n"," – from_json()\n"," – to_json()\n"," – json_tuple()\n"," – get_json_object()\n"," – schema_of_json()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-17-81bf515c7309>, line 7)","output_type":"error","traceback":["\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-81bf515c7309>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    path = 'C:\\Users\\QQ\\Documents\\interaction_ids.csv'\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"]}],"source":["import numpy as np\n","letters = [x for x in \"abdce\"]\n","numbers = [x for x in range(5)]\n","a = np.array(letters)\n","b = np.array(numbers)\n","\n","path = 'C:\\Users\\QQ\\Documents\\interaction_ids.csv'\n","import pandas as pd\n","df = pd.read_csv(path)\n","df\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"SparkSet","notebookOrigID":1279660130152254,"widgets":{"SCHEMA_STATUS":{"currentValue":"LOADED","nuid":"0b882202-204c-4d41-b8f5-9204fa5d46c6","widgetInfo":{"defaultValue":"LOADED","label":null,"name":"SCHEMA_STATUS","options":{"choices":["LOADED","NOT LOADED"],"widgetType":"dropdown"},"widgetType":"dropdown"}}}},"interpreter":{"hash":"de2447fb81e3bdedb6496b06bacdb2c52fb1921ab17eb741c5f37bae09bc662a"},"kernelspec":{"display_name":"Python 3.9.10 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}
