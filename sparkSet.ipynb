{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string as s\n",
    "import pandas as pd\n",
    "import os.path,inspect, re \n",
    "import random as r\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "import pathlib\n",
    "import time\n",
    "import os\n",
    "import zipfile\n",
    "import concurrent.futures\n",
    "\n",
    "from pyspark.sql import types as T,functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta \n",
    "import pyspark\n",
    "from pyspark import SparkContext ,SparkConf, SparkSession, SparkContext\n",
    "from pyspark.sql import SQLContext  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ( SparkConf()\n",
    "        .setMaster(\"local\")\n",
    "        .setAppName(\"MyApp\")\n",
    "        .set(\"spark.executor.memory\",\"6g\")\n",
    "        .set(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")\n",
    "        .set('spark.sql.execution.arrow.pyspark.enabled' , False)\n",
    "        .set('spark.sql.caseSensitive', True))\n",
    "        \n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DirSet:\n",
    "    def __init__(self, directory=None):\n",
    "        self.dir = directory\n",
    "        \n",
    "    def countDir(self):\n",
    "        \n",
    "        dcFiles = dbutils.fs.ls(self.dir)\n",
    "        df = spark.createDataFrame(sc.parallelize(dcFiles))\n",
    "        count = df.count()\n",
    "        displayHTML(\"DIRECTORY SET AS: ...  %s\" % self.dir)\n",
    "        displayHTML(\"TOTAL OF FILES FOUND AFTER SCANNING : ... %s\" % count)   \n",
    "\n",
    "    def graphDir(self):\n",
    "        \n",
    "        dcFiles = dbutils.fs.ls(self.dir)\n",
    "        df = spark.createDataFrame(sc.parallelize(dcFiles))\n",
    "        df = df.sort(df.size)\n",
    "        count = df.count()\n",
    "        display(df)\n",
    "\n",
    "\n",
    "    def descDir(self):\n",
    "        #origin\n",
    "        dcFiles = dbutils.fs.ls(self.dc)\n",
    "        df = spark.createDataFrame(sc.parallelize(dcFiles))\n",
    "        df = df.describe()\n",
    "        display(df)\n",
    "\n",
    "    def filesDir(self):\n",
    "        #origin\n",
    "        dcFiles = dbutils.fs.ls(self.dir)\n",
    "        return dcFiles\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de2447fb81e3bdedb6496b06bacdb2c52fb1921ab17eb741c5f37bae09bc662a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
