{"cells":[{"cell_type":"markdown","metadata":{},"source":["# REFERENCIAS"]},{"cell_type":"markdown","metadata":{},"source":["### GENERAL IMPORTS"]},{"cell_type":"code","execution_count":1,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d9d60deb-380e-4f9c-b29d-d0bfb8ef35a4","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"]}],"source":["\n","import json\n","import bson as bson\n","import string as s\n","import pandas as pd\n","import os.path,inspect, re \n","import random as r\n","from types import SimpleNamespace\n","import sys\n","import pathlib\n","import time\n","import os\n","import zipfile\n","import concurrent.futures\n","from pymongo import MongoClient\n"]},{"cell_type":"markdown","metadata":{},"source":["## DIR FUNCTIONS"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1be7b5a5-91bf-4184-9ea7-f96912fe43f0","showTitle":false,"title":""}},"outputs":[],"source":["\n","        \n","def countDir(self): \n","    dcFiles = dbutils.fs.ls(self.DIR)\n","    df = spark.createDataFrame(sc.parallelize(dcFiles))\n","    count = df.count()\n","    displayHTML(\"DIRECTORY SET AS: ...  %s\" % self.DIR)\n","    displayHTML(\"TOTAL OF FILES FOUND AFTER SCANNING : ... %s\" % count)   \n","\n","def graphDir(self):\n","    dcFiles = dbutils.fs.ls(self.DIR)\n","    df = spark.createDataFrame(sc.parallelize(dcFiles))\n","    df = df.sort(df.size)\n","    count = df.count()\n","    display(df)\n","\n","def descDir(self):\n","    #origin\n","    dcFiles = dbutils.fs.ls(self.dc)\n","    df = spark.createDataFrame(sc.parallelize(dcFiles))\n","    df = df.describe()\n","    display(df)\n","\n","def filesDir(self):\n","    #origin\n","    dcFiles = dbutils.fs.ls(self.DIR)\n","    return dcFiles       \n","    \n","\n","\n","def getInterByIdSingle(self,id=None):\n","    col = st.findByMongo(COLLECTION='collection')\n","    col_filter = [\n","        {\n","            '$match': {\n","                '_id': '%s' % id\n","            }\n","        }\n","                ]\n","    result = col.aggregate(col_filter)\n","    result = list(result)\n","    #UTILIZAR ALGUMA FORMA DE PERSISTÊNCIA EM MEMÓRIA PARA ACUMULAR OS RESULTADOS\n","    self.LIST.append(result)\n","    size = len(self.INTER_LIST)\n","    print(size)\n","\n","def threadExecute(self, FUNC = None, ITERABLE=None):\n","    \n","    #THREAD POOL EXECUTOR\n","    with concurrent.futures.ThreadPoolExecutor() as executor:\n","    futures = []\n","    #ITERABLE\n","    for ITEM in ITERABLE:\n","        #FUNCTION\n","        futures.append(executor.submit(FUNC, PARAM=ITEM))\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"SparkSet","notebookOrigID":1279660130152254,"widgets":{"SCHEMA_STATUS":{"currentValue":"LOADED","nuid":"0b882202-204c-4d41-b8f5-9204fa5d46c6","widgetInfo":{"defaultValue":"LOADED","label":null,"name":"SCHEMA_STATUS","options":{"choices":["LOADED","NOT LOADED"],"widgetType":"dropdown"},"widgetType":"dropdown"}}}},"interpreter":{"hash":"de2447fb81e3bdedb6496b06bacdb2c52fb1921ab17eb741c5f37bae09bc662a"},"kernelspec":{"display_name":"Python 3.9.10 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}
