{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "import sys\n",
    "import os.path\n",
    "import bson as b\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import types as T \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta \n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext  \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = (SparkConf()\\\n",
    "    .setMaster('local')\\\n",
    "    .setAppName('sparkApp')\\\n",
    "    .set('spark.Builder.enableHiveSupport',True)\\\n",
    "    .set(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")\\\n",
    "    .set('spark.sql.execution.arrow.pyspark.enabled' , False)\\\n",
    "    .set('spark.sql.caseSensitive', True))\n",
    "sc = SparkContext(conf=conf) \n",
    " \n",
    "spark = SparkSession(sc)\n",
    "\n",
    "uri_string = \"mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&directConnection=true&ssl=false\"\n",
    "dbName = 'local'\n",
    "colName = 'STP'\n",
    "client = MongoClient(uri_string)\n",
    "database = client.get_database(dbName)\n",
    "col = database.get_collection(colName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PYMONGO METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def colCount():\n",
    "    colCount =  col.find({},{\"_id\"}).count()\n",
    "    return colCount\n",
    "\n",
    "def colFind(limit=\"\"):\n",
    "    if limit == \"\":\n",
    "        colList = [ x for x in col.find()]\n",
    "    else:\n",
    "        colList = [ x for x in col.find().limit(limit)]\n",
    "    return(colList)\n",
    "\n",
    "def colRawFind(limit=\"\"):\n",
    "    if limit == \"\":\n",
    "        colList = [ x for x in col.find_raw_batches()]\n",
    "    else:\n",
    "        colList = [ b.decode_all(x) for x in col.find_raw_batches().limit(limit)]\n",
    "    return(colList)\n",
    "\n",
    "def colFindID():\n",
    "    idList = [str(x).split(\"'\",4)[3] for x in col.find({},{\"_id\":1}).limit(5)]\n",
    "    return(idList)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PYSPARK METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0a4b30dcf02a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDATA_SCHEMA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.mongodb.input.sampleSize'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"database\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loca\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.mongodb.input.collection\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"STP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_SCHEMA = spark.read\\\n",
    "                .format( \"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "                .option('spark.mongodb.input.sampleSize', 100)\\\n",
    "                .option(\"database\", \"loca\")\\\n",
    "                .option(\"spark.mongodb.input.collection\", \"STP\")\\\n",
    "                .load().schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PARALLEL METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxRunWorkerTask(ID=None):\n",
    "    #TRANSFORMATION FUNC\n",
    "\n",
    "    QUERY = db.client.get_database(\"local\").get_collection(\"SPT\").find({ 'match' : {'_id': ID}})     \n",
    "    for x in QUERY:\n",
    "        RESULT_LIST.append(x)\n",
    "    \n",
    "\n",
    "def maxRunWorker(max_workers=4, mode=\"submit\"):    \n",
    "    ID_LIST = colFindID()\n",
    "    T0 = datetime.now()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        if mode == \"submit\":\n",
    "            for x in ID_LIST:\n",
    "                executor.submit(maxRunWorkerTask, ID=x)\n",
    "        else:\n",
    "            executor.map(maxRunWorkerTask, ID_LIST)\n",
    "        \n",
    "    T1 = datetime.now()\n",
    "    print(f\" QUERY_TIME = {(T1-T0).seconds} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de2447fb81e3bdedb6496b06bacdb2c52fb1921ab17eb741c5f37bae09bc662a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
